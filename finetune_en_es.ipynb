{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8050552f",
   "metadata": {},
   "source": [
    "# Englishâ†’Spanish LoRA/QLoRA Fine-Tuning\n",
    "\n",
    "Fine-tune a causal language model on `loresiensis/corpus-en-es` with parameter-efficient adapters. Toggle QLoRA via the config to enable 4-bit training on larger checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628f34a",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "- Install dependencies (transformers, datasets, peft, bitsandbytes, accelerate, evaluate, sacrebleu).\n",
    "- Configure the base checkpoint, dataset, and LoRA/QLoRA hyperparameters.\n",
    "- Load and tokenize the Englishâ†’Spanish corpus with an instruction-style prompt.\n",
    "- Prepare a LoRA-wrapped model (optionally quantized with QLoRA).\n",
    "- Train with `Trainer`, track SacreBLEU, and persist adapters/merged weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ba095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets peft bitsandbytes accelerate evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3decded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command configures the notebook to use the virtual environment's kernel.\n",
    "# It's typically run once from the terminal, not within the notebook itself.\n",
    "# !python -m ipykernel install --user --name=.venv --display-name \"Python (.venv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1042517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f30d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base_model\": \"HuggingFaceTB/SmolLM-135M\",\n",
    "    \"dataset_name\": \"loresiensis/corpus-en-es\",\n",
    "    \"output_dir\": \"outputs/smol-lora\",\n",
    "    \"use_qlora\": False,\n",
    "    \"max_train_samples\": 100_000,\n",
    "    \"max_eval_samples\": 2500,\n",
    "    \"source_lang_key\": \"EN\",\n",
    "    \"target_lang_key\": \"ES\",\n",
    "    \"max_length\": 512,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 1000,\n",
    "    \"eval_steps\": 5000,\n",
    "    \"save_steps\": 5000\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f1dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"base_model\"], padding_side=\"right\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# raw_datasets: DatasetDict = load_dataset(config[\"dataset_name\"])\n",
    "\n",
    "# if config[\"max_train_samples\"]:\n",
    "#     raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(min(len(raw_datasets[\"train\"]), config[\"max_train_samples\"])))\n",
    "# if config[\"max_eval_samples\"]:\n",
    "#     raw_datasets[\"test\"] = raw_datasets[\"test\"].select(range(min(len(raw_datasets[\"test\"]), config[\"max_eval_samples\"])))\n",
    "\n",
    "# prompt_template = \"Translate the following English sentence into natural Spanish.\\nEnglish: {src}\\nSpanish:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f17a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "def preprocess_batch(batch: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "    en_batch = batch['EN']\n",
    "    es_batch = batch['ES']\n",
    "    # translations = batch[\"translation\"]  # each item is {\"en\": ..., \"es\": ...}\n",
    "    en_texts = en_batch\n",
    "    es_texts = es_batch\n",
    "\n",
    "    prompts = [\n",
    "        \"Translate the following English text into Spanish.\\n\\n\"\n",
    "        f\"English: {en}\\n\"\n",
    "        \"Spanish:\"\n",
    "        for en in en_texts\n",
    "    ]\n",
    "\n",
    "    full_texts = [\n",
    "        p + \" \" + es + tokenizer.eos_token\n",
    "        for p, es in zip(prompts, es_texts)\n",
    "    ]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Create labels and mask out the prompt part, so only Spanish tokens incur loss\n",
    "    labels = []\n",
    "    for full_text, prompt, es in zip(full_texts, prompts, es_texts):\n",
    "        # full sequence\n",
    "        input_ids = tokenized[\"input_ids\"][len(labels)]\n",
    "        # get tokenized prompt (no eos, no target)\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        # initialize labels as a copy\n",
    "        label_ids = input_ids.copy()\n",
    "\n",
    "        # mask out prompt tokens (set to -100 so they are ignored by loss)\n",
    "        prompt_len = len(prompt_ids)\n",
    "        label_ids[:prompt_len] = [-100] * min(prompt_len, len(label_ids))\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    tokenized['length'] = [len(ids) for ids in tokenized['input_ids']]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc50c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_dataset(config[\"dataset_name\"])\n",
    "\n",
    "# # raw_datasets has only \"train\" split; create a small validation split\n",
    "# split = raw_datasets[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "# train_dataset = split[\"train\"]\n",
    "# eval_dataset = split[\"test\"]\n",
    "\n",
    "# print(train_dataset[0])\n",
    "# train_dataset = train_dataset.map(\n",
    "#     preprocess_batch,\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset.column_names,\n",
    "# )\n",
    "\n",
    "# eval_dataset = eval_dataset.map(\n",
    "#     preprocess_batch,\n",
    "#     batched=True,\n",
    "#     remove_columns=eval_dataset.column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3782d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 122441 duplicates. New shape: (877559, 2)\n",
      "Removed 5 duplicates. New shape: (1995, 2)\n",
      "{'EN': 'Oral hypoglycaemic agents (OHA), monoamine oxidase inhibitors (MAOI), non-selective beta- blocking agents, angiotensin converting enzyme (ACE) inhibitors, salicylates, alcohol, anabolic steroids and sulphonamides.', 'ES': 'Hipoglucemiantes orales (HO), inhibidores de la monoamino oxidasa (IMAO), betabloqueantes no selectivos, inhibidores de la enzima conversora de la angiotensina (IECA), salicilatos, alcohol, esteroides anabolizantes y sulfonamidas.', '__index_level_0__': 41764}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:29<00:00, 3348.56 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1995/1995 [00:00<00:00, 3402.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-es\")\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "# test_df = ds[\"test\"].to_pandas()\n",
    "val_df = ds[\"validation\"].to_pandas()\n",
    "\n",
    "def preprocess_text(train_df, max_size=-1):\n",
    "    # {'en': \"It was the asbestos in here, that's what did it!\", 'es': 'Fueron los asbestos aquÃ­. Â¡Eso es lo que ocurriÃ³!'}\n",
    "    train_df['EN'] = train_df['translation'].apply(lambda x: x['en'])\n",
    "    train_df['ES'] = train_df['translation'].apply(lambda x: x['es'])\n",
    "    # remove the original 'translation' column\n",
    "    train_df = train_df.drop(columns=['translation'])\n",
    "    # remove duplicate rows based on the 'en' column (keep the first occurrence)\n",
    "    before = len(train_df)\n",
    "    # remove nulls\n",
    "    train_df = train_df.dropna(subset=['EN'])\n",
    "    # apply strip\n",
    "    train_df[\"EN\"] = train_df[\"EN\"].str.strip()\n",
    "    train_df = train_df.drop_duplicates(subset=\"EN\", keep=\"first\").reset_index(drop=True)\n",
    "    after = len(train_df)\n",
    "    print(f\"Removed {before - after} duplicates. New shape: {train_df.shape}\")\n",
    "    if max_size > 0:\n",
    "        train_df = train_df.sample(max_size, random_state=42) \n",
    "    # to datasets\n",
    "    \n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    return train_ds\n",
    "\n",
    "train_dataset = preprocess_text(train_df, config['max_train_samples'])\n",
    "eval_dataset = preprocess_text(val_df)\n",
    "# test_df = preprocess_text(test_df)\n",
    "print(train_dataset[0])\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0648bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(train_dataset['length'], bins=30)\n",
    "# plt.title('Distribution of tokenized sequence lengths')\n",
    "# plt.xlabel('Length')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baa040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def build_prompt(text: str) -> str:\n",
    "# #     return prompt_template.format(src=text.strip())\n",
    "\n",
    "# def tokenize_function(example: Dict[str, str]) -> Dict[str, List[int]]:\n",
    "#     source_text = example[config[\"source_lang_key\"]]\n",
    "#     target_text = example[config[\"target_lang_key\"]]\n",
    "#     prompt = build_prompt(source_text)\n",
    "#     full_text = f\"{prompt} {target_text.strip()}\"\n",
    "\n",
    "#     tokenized = tokenizer(full_text, truncation=True, max_length=config[\"max_length\"])\n",
    "#     labels = tokenized[\"input_ids\"].copy()\n",
    "#     prompt_ids = tokenizer(prompt, truncation=True, max_length=config[\"max_length\"])[\"input_ids\"]\n",
    "#     labels[: len(prompt_ids)] = [-100] * len(prompt_ids)\n",
    "#     tokenized[\"labels\"] = labels\n",
    "#     return tokenized\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(\n",
    "#     tokenize_function,\n",
    "#     remove_columns=raw_datasets[\"train\"].column_names,\n",
    "#     desc=\"Tokenizing dataset\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ee935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     label_pad_token_id=-100,\n",
    "# )\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf7cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred              # logits: (batch, seq_len, vocab_size)\n",
    "    pred_ids = np.argmax(logits, axis=-1)   # -> (batch, seq_len)\n",
    "\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    pred_strs = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_strs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    references = [[ref] for ref in label_strs]\n",
    "    bleu = metric.compute(predictions=pred_strs, references=references)\n",
    "    return {\"sacrebleu\": bleu[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ded46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model_kwargs = {}\n",
    "    if config[\"use_qlora\"]:\n",
    "        compute_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "        )\n",
    "        model_kwargs[\"quantization_config\"] = quant_config\n",
    "        model_kwargs[\"device_map\"] = \"auto\"\n",
    "    else:\n",
    "        model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    model = AutoModelForCausalLM.from_pretrained(config[\"base_model\"], **model_kwargs)\n",
    "    if config[\"use_qlora\"]:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "193e6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 136,358,208 || trainable%: 1.3517\n"
     ]
    }
   ],
   "source": [
    "model = init_model()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e34901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    num_train_epochs=config[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_ratio=config[\"warmup_ratio\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=not torch.cuda.is_available(),\n",
    ")\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=config[\"output_dir\"],\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=config[\"num_train_epochs\"],\n",
    "#     per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "#     per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
    "#     gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "#     learning_rate=config[\"learning_rate\"],\n",
    "#     warmup_ratio=config[\"warmup_ratio\"],\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     logging_steps=50,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=10,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=500,\n",
    "#     save_total_limit=3,\n",
    "#         # ðŸ”‘ The important bits:\n",
    "#     bf16=torch.cuda.is_available(),        # training in bf16\n",
    "#     bf16_full_eval=torch.cuda.is_available(),  # eval ALSO in bf16\n",
    "\n",
    "#     report_to=\"none\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     seed=42,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dcb3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc18f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14838' max='31250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14838/31250 2:07:16 < 2:20:47, 1.94 it/s, Epoch 2.37/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.048000</td>\n",
       "      <td>2.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.957300</td>\n",
       "      <td>1.965820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be6d72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='499' max='499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [499/499 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8940869569778442,\n",
       " 'eval_runtime': 23.4725,\n",
       " 'eval_samples_per_second': 84.993,\n",
       " 'eval_steps_per_second': 21.259,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6da690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/smol-lora/lora-adapter'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_dir = os.path.join(config[\"output_dir\"], \"lora-adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "adapter_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3718936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: merge adapters into the base model for export\n",
    "def merge_and_save(base_model_dir: str, adapter_dir: str, merged_dir: str, dtype=torch.float16):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_dir, torch_dtype=dtype)\n",
    "    peft_model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "merge_and_save(config[\"base_model\"], adapter_dir, os.path.join(config[\"output_dir\"], \"merged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a355eb8-5e85-4b9c-b032-6bc3731a16bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
