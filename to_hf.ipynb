{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e47d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ `num_layers` is part of LlamaModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/leo/project/distil-research/smol/llama.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'en-es'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "from llama import LlamaForCausalLM, MultiTaskLlamaForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"en-es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49397eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi-task-frozen-28-task-layers-2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "name = \"./exp-data/runs/multi-task-True-frozen-28-task-layers-2\"\n",
    "\n",
    "\n",
    "num_frozen_layers = int(name.split(\"frozen-\")[1].split(\"-\")[0])\n",
    "num_task_layers = int(name.split(\"task-layers-\")[1])\n",
    "max_length = 512\n",
    "effective_batch_size = 32\n",
    "batch_size = 32\n",
    "accumulate_grad_batches = effective_batch_size // batch_size\n",
    "weight_decay = 0.01\n",
    "epochs = 5\n",
    "learning_rate = 5e-5\n",
    "grad_clip_val = 1.0\n",
    "max_train_sample_size = 100_000\n",
    "max_val_sample_size = 10_000\n",
    "model_name_or_path = \"HuggingFaceTB/SmolLM-135M\"\n",
    "use_lora = \"multi-task\" not in name\n",
    "use_qlora = \"qlora\" in name\n",
    "is_multi_task = (not use_lora)  and (not use_qlora)\n",
    "method_name = \"multi-task\" if is_multi_task else \"lora-qlora\"\n",
    "run_name = f\"{method_name}-frozen-{num_frozen_layers}-task-layers-{num_task_layers}\"\n",
    "os.environ.setdefault(\"WANDB_NAME\", run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c1a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "config.num_frozen_layers = num_frozen_layers\n",
    "config.num_task_layers = num_task_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5b3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_lora and not use_qlora:\n",
    "    pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = MultiTaskLlamaForCausalLM(config)\n",
    "    model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    for param in model.model.parameters():\n",
    "        param.requires_grad = False\n",
    "elif use_lora or use_qlora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    if use_qlora:\n",
    "        # For QLoRA, quantize the model weights to 4-bit\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path, quantization_config=quantization_config)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    else:\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"Model updated with LoRA/QLoRA for parameter-efficient fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68f1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from exp-data/runs/multi-task-True-frozen-28-task-layers-2/checkpoints/best-epoch=09-val_loss=2.2497.ckpt with strict=True.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path(name + \"/checkpoints\")\n",
    "ckpt_files = sorted(ckpt_dir.glob(\"*.ckpt\"))\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(f\"No .ckpt files found in {ckpt_dir}\")\n",
    "\n",
    "ckpt_path = ckpt_files[-1]\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "# remove model. prefix if present\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"model.\"):\n",
    "        new_key = k[len(\"model.\"):]\n",
    "    else:\n",
    "        new_key = k\n",
    "    new_state_dict[new_key] = v\n",
    "state_dict = new_state_dict\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(f\"Loaded checkpoint from {ckpt_path} with strict=True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfabedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in pretrained model: 134,515,008\n",
      "Total number of parameters in multi-task model: 134,515,008\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(pretrained_model)\n",
    "print(f\"Total number of parameters in pretrained model: {total_params:,}\")\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total number of parameters in multi-task model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdfc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def merge_multitask_layers(multitask_model: MultiTaskLlamaForCausalLM) -> LlamaForCausalLM:\n",
    "    \"\"\"\n",
    "    Merges the task_layers from a MultiTaskLlamaForCausalLM instance into its model.layers,\n",
    "    and returns a LlamaForCausalLM instance with the merged layers.\n",
    "\n",
    "    Args:\n",
    "        multitask_model (MultiTaskLlamaForCausalLM): The multitask model instance.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: The merged LlamaForCausalLM instance.\n",
    "    \"\"\"\n",
    "    if multitask_model.task_layers is not None and len(multitask_model.task_layers) > 0:\n",
    "        merged_layers = list(multitask_model.model.layers) + list(multitask_model.task_layers)\n",
    "        multitask_model.model.layers = nn.ModuleList(merged_layers)\n",
    "        multitask_model.model.num_layers = len(merged_layers)\n",
    "    # Create a new LlamaForCausalLM with the merged model\n",
    "    merged_model = LlamaForCausalLM(multitask_model.config)\n",
    "    merged_model.model = multitask_model.model\n",
    "    merged_model.lm_head = multitask_model.lm_head\n",
    "    return merged_model\n",
    "\n",
    "model = merge_multitask_layers(model)\n",
    "model.config.num_hidden_layers = len(model.model.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d38ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ff3442f9e44a519555d616000d3100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07245264784fecaea1d34767f217c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09de34101f934663a00104e6c2fa6b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-28-task-layers-2/model.safetensors:  23%|##3       |  126MB /  538MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model and tokenizer to https://huggingface.co/leobitz/multi-task-True-frozen-28-task-layers-2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "hf_repo_id = name.split(\"./exp-data/runs/\")[-1]\n",
    "hf_repo_id = f\"leobitz/{hf_repo_id}\"\n",
    "hf_token = \"\"\n",
    "if hf_repo_id is None:\n",
    "    raise EnvironmentError(\"Set HF_REPO_ID env var to '<username>/<repo>' before pushing.\")\n",
    "\n",
    "artifact_dir = Path(\"hf-artifacts\") / Path(run_name).name\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(artifact_dir)\n",
    "tokenizer.save_pretrained(artifact_dir)\n",
    "model.config.save_pretrained(artifact_dir)\n",
    "\n",
    "api = HfApi(token=hf_token)\n",
    "create_repo(repo_id=hf_repo_id, exist_ok=True, token=hf_token)\n",
    "api.upload_folder(\n",
    "    repo_id=hf_repo_id,\n",
    "    folder_path=str(artifact_dir),\n",
    "    path_in_repo=\".\",\n",
    "    commit_message=f\"Upload {run_name}\",\n",
    ")\n",
    "print(f\"Pushed model and tokenizer to https://huggingface.co/{hf_repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
