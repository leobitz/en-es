{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b9e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora-qlora-frozen-16-task-layers-2-aug-0.0\n",
      "Model updated with LoRA/QLoRA for parameter-efficient fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback, TrainerCallback\n",
    "from transformers import LlamaForCausalLM\n",
    "# from llama import LlamaForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import set_seed\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers.trainer_callback import PrinterCallback\n",
    "set_seed(42)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"en-es\")\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train EN->ES translation model\")\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--num-frozen-layers\", type=int, default=8, help=\"Count of frozen base layers\")\n",
    "#     parser.add_argument(\"--num-task-layers\", type=int, default=2, help=\"Number of task-specific layers\")\n",
    "#     parser.add_argument(\"--use-lora\", action=\"store_true\", help=\"Enable LoRA fine-tuning\")\n",
    "#     parser.add_argument(\"--use-qlora\", action=\"store_true\", help=\"Enable QLoRA fine-tuning\")\n",
    "#     parser.add_argument(\"--aug-size\", type=float, default=0.1, help=\"percentage of augmented data to use compared to original data\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# # exmaple\n",
    "# # python train_hf.py --epochs 5 --num-frozen-layers 10 --num-task-layers 4 --use-lora\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "num_frozen_layers = 16\n",
    "num_task_layers = 2\n",
    "\n",
    "max_length = 512\n",
    "effective_batch_size = 64\n",
    "batch_size = 64\n",
    "accumulate_grad_batches = effective_batch_size // batch_size\n",
    "weight_decay = 0.01\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "grad_clip_val = 1.0\n",
    "max_train_sample_size = 100_000\n",
    "max_val_sample_size = 10_000\n",
    "model_name_or_path = \"HuggingFaceTB/SmolLM-135M\"\n",
    "use_lora = True\n",
    "use_qlora = False\n",
    "aug_size = 0.0\n",
    "\n",
    "is_multi_task = (not use_lora)  and (not use_qlora)\n",
    "method_name = \"multi-task\" if is_multi_task else \"lora-qlora\"\n",
    "run_name = f\"{method_name}-frozen-{num_frozen_layers}-task-layers-{num_task_layers}-aug-{aug_size}\"\n",
    "print(run_name)\n",
    "os.environ.setdefault(\"WANDB_NAME\", run_name)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "assert num_task_layers + num_frozen_layers <= config.num_hidden_layers, f\"num_task_layers + num_frozen_layers must equal or less than {config.num_hidden_layers}\"\n",
    "\n",
    "\n",
    "if not use_lora and not use_qlora:\n",
    "    pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    new_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    new_config.num_hidden_layers = num_task_layers + num_frozen_layers\n",
    "    model = LlamaForCausalLM(new_config)\n",
    "\n",
    "    random_task_layers = [copy.deepcopy(layer) for layer in model.model.layers[-num_task_layers:]]\n",
    "\n",
    "    model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "    # Overwrite the last num_task_layers layers with the random ones\n",
    "    for i in range(num_task_layers):\n",
    "        model.model.layers[-num_task_layers + i] = random_task_layers[i]\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if num_task_layers > 0:\n",
    "        # Make the last num_task_layers layers trainable\n",
    "        for layer in model.model.layers[-num_task_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "elif use_lora or use_qlora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    if use_qlora:\n",
    "        # For QLoRA, quantize the model weights to 4-bit\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path, quantization_config=quantization_config)\n",
    "\n",
    "        new_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        new_config.num_hidden_layers = num_task_layers + num_frozen_layers\n",
    "        model = LlamaForCausalLM(new_config)\n",
    "        \n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    else:\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "        new_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        new_config.num_hidden_layers = num_task_layers + num_frozen_layers\n",
    "        model = LlamaForCausalLM(new_config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"Model updated with LoRA/QLoRA for parameter-efficient fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac80ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in pretrained model: 134,515,008\n",
      "Total number of parameters in multi-task model: 93,139,776\n",
      "Trainable parameters: 1,105,920\n",
      "Non-trainable parameters: 92,033,856\n",
      "Using 0 augmented samples out of 69324 available.\n",
      "Number of samples: 100000\n",
      "Number of validation samples: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(pretrained_model)\n",
    "print(f\"Total number of parameters in pretrained model: {total_params:,}\")\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total number of parameters in multi-task model: {total_params:,}\")\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_non_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "trainable_params = count_trainable_parameters(model)\n",
    "non_trainable_params = count_non_trainable_parameters(model)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "\n",
    "template = \"English: {english_text} Spanish: {spanish_text} <|END|>\"\n",
    "\n",
    "# exp-data/en-es-train-val.parquet\n",
    "if not os.path.exists(\"exp-data/en-es-train-val.parquet\"):\n",
    "    data_df = pd.read_parquet(\"exp-data/en-es.parquet\")\n",
    "    train_df = data_df[data_df[\"split\"] == \"train\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    train_df['length'] = train_df.apply(lambda x: len(tokenizer.encode(template.format(english_text=x['EN'], spanish_text=x['ES']))), axis=1)\n",
    "    train_df = train_df[train_df['length'] <= max_length].reset_index(drop=True)\n",
    "    print(f\"Number of training samples after length filtering: {len(train_df)}\")\n",
    "    # split train_df 80-20 into train and validation\n",
    "    split_idx = int(0.8 * len(train_df))\n",
    "    train_df_split = train_df.iloc[:split_idx].reset_index(drop=True)\n",
    "    val_df_split = train_df.iloc[split_idx:].reset_index(drop=True)\n",
    "    train_df = train_df_split\n",
    "    val_df = val_df_split\n",
    "    train_df = train_df.sample(min(max_train_sample_size, len(train_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df = val_df.sample(min(max_val_sample_size, len(val_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df['split'] = 'val'\n",
    "    train_df['split'] = 'train'\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")\n",
    "    # to parquet\n",
    "    train_proc_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "    train_proc_df.to_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "else:\n",
    "    train_proc_df = pd.read_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "    synthetic_df = pd.read_parquet(\"exp-data/synthetic-en-es-data.parquet\")\n",
    "    original_synthetic_size = len(synthetic_df)\n",
    "    size = int(len(train_proc_df) * aug_size)\n",
    "    size = min(size, len(synthetic_df))\n",
    "    synthetic_df = synthetic_df.sample(n=size, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Using {len(synthetic_df)} augmented samples out of {original_synthetic_size} available.\")\n",
    "    train_proc_df = pd.concat([train_proc_df, synthetic_df]).reset_index(drop=True)\n",
    "    \n",
    "    train_df = train_proc_df[train_proc_df[\"split\"] == \"train\"]\n",
    "    train_df = train_df.sample(min(max_train_sample_size, len(train_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df = train_proc_df[train_proc_df[\"split\"] == \"val\"]\n",
    "    val_df = val_df.sample(min(max_val_sample_size, len(val_df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # remove duplicates in train_df based on EN and ES columns\n",
    "    train_df = train_df.drop_duplicates(subset=[\"EN\", \"ES\"]).reset_index(drop=True)\n",
    "    val_df = val_df.drop_duplicates(subset=[\"EN\", \"ES\"]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")\n",
    "\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.template = \"English: {} Spanish: \"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_text = self.df.iloc[idx][\"EN\"].strip()\n",
    "        es_text = self.df.iloc[idx][\"ES\"].strip()\n",
    "        prompt = self.template.format(en_text)\n",
    "        # full_text = template.format(english_text=en_text, spanish_text=es_text)\n",
    "        \n",
    "        es_text = es_text + \" <|END|>\"\n",
    "        # Tokenize prompt and full text\n",
    "        prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "        es_ids = self.tokenizer(es_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        # Labels: -100 for prompt, actual tokens for output\n",
    "        labels = prompt_ids + es_ids\n",
    "        input_ids = prompt_ids + es_ids\n",
    "\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def dynamic_padding_collator(tokenizer: AutoTokenizer):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def _collate(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        if not batch:\n",
    "            return {\n",
    "                \"input_ids\": torch.empty(0, dtype=torch.long),\n",
    "                \"attention_mask\": torch.empty(0, dtype=torch.long),\n",
    "                \"labels\": torch.empty(0, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "        max_len = max(item[\"input_ids\"].size(0) for item in batch)\n",
    "        batch_size = len(batch)\n",
    "        input_ids = torch.full((batch_size, max_len), pad_token_id, dtype=torch.long)\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "\n",
    "        for idx, item in enumerate(batch):\n",
    "            seq_len = item[\"input_ids\"].size(0)\n",
    "            input_ids[idx, :seq_len] = item[\"input_ids\"]\n",
    "            attention_mask[idx, :seq_len] = 1\n",
    "            label_len = item[\"labels\"].size(0)\n",
    "            labels[idx, :label_len] = item[\"labels\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    return _collate\n",
    "\n",
    "train_dataset = TranslationDataset(train_df, tokenizer, max_length=max_length)\n",
    "val_dataset = TranslationDataset(val_df, tokenizer, max_length=max_length)\n",
    "data_collator = dynamic_padding_collator(tokenizer)\n",
    "\n",
    "\n",
    "preview_samples = []\n",
    "if len(val_df) > 0:\n",
    "    preview_samples = [\n",
    "        {\"EN\": row[\"EN\"].strip(), \"ES\": row[\"ES\"].strip()}\n",
    "        for _, row in val_df.sample(n=min(3, len(val_df)), random_state=123).iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "class SampleTranslationCallback(TrainerCallback):\n",
    "    \"\"\"Logs a few reference/prediction pairs after each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, samples, max_new_tokens=80):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = samples\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if not self.samples:\n",
    "            return\n",
    "\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"\\n=== Sample translations after epoch {int(state.epoch or 0)} ===\")\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(self.samples, start=1):\n",
    "                prompt = f\"English: {sample['EN']} Spanish: \"\n",
    "                encoded = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "                generated = model.generate(\n",
    "                    **encoded,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "\n",
    "                input_len = encoded[\"input_ids\"].shape[-1]\n",
    "                new_tokens = generated[0, input_len:]\n",
    "                pred_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "                pred_text = pred_text.split(\"<|END|>\")[0].strip()\n",
    "\n",
    "                print(f\"Sample {idx}\")\n",
    "                print(f\"EN : {sample['EN']}\")\n",
    "                print(f\"ES*: {sample['ES']}\")\n",
    "                print(f\"ES^: {pred_text if pred_text else '<empty>'}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9e208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text: English: Despite some infringements of human rights, which have been rightly condemned by the international community, the country has remained, in essence, under the rule of law. It has been able to do so largely because of the dynamism of civil society and of the independence of its judicial system. Spanish: A pesar de algunas violaciones de los derechos humanos, justamente denunciados por la comunidad internacional, sigue siendo básicamente un Estado de Derecho, en especial gracias al dinamismo de su sociedad civil y a la independencia de su justicia. <|END|>\n",
      "Decoded text: English: Despite some infringements of human rights, which have been rightly condemned by the international community, the country has remained, in essence, under the rule of law. It has been able to do so largely because of the dynamism of civil society and of the independence of its judicial system. Spanish: A pesar de algunas violaciones de los derechos humanos, justamente denunciados por la comunidad internacional, sigue siendo básicamente un Estado de Derecho, en especial gracias al dinamismo de su sociedad civil y a la independencia de su justicia. <|END|>\n",
      "Decoded label text: English: Despite some infringements of human rights, which have been rightly condemned by the international community, the country has remained, in essence, under the rule of law. It has been able to do so largely because of the dynamism of civil society and of the independence of its judicial system. Spanish: A pesar de algunas violaciones de los derechos humanos, justamente denunciados por la comunidad internacional, sigue siendo básicamente un Estado de Derecho, en especial gracias al dinamismo de su sociedad civil y a la independencia de su justicia. <|END|>\n",
      "Match: True\n",
      "Label Match: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_447018/2502896452.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleobitz\u001b[0m (\u001b[33mbitzbrain\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/en-es/wandb/run-20251126_124101-zfqvlnfu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bitzbrain/en-es/runs/zfqvlnfu' target=\"_blank\">lora-qlora-frozen-16-task-layers-2-aug-0.0</a></strong> to <a href='https://wandb.ai/bitzbrain/en-es' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bitzbrain/en-es' target=\"_blank\">https://wandb.ai/bitzbrain/en-es</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bitzbrain/en-es/runs/zfqvlnfu' target=\"_blank\">https://wandb.ai/bitzbrain/en-es/runs/zfqvlnfu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 31.36 GiB of which 5.31 GiB is free. Including non-PyTorch memory, this process has 26.04 GiB memory in use. Of the allocated memory 24.96 GiB is allocated by PyTorch, and 512.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     63\u001b[39m trainer.add_callback(\n\u001b[32m     64\u001b[39m     EarlyStoppingCallback(\n\u001b[32m     65\u001b[39m         early_stopping_patience=\u001b[32m2\u001b[39m,\n\u001b[32m     66\u001b[39m         early_stopping_threshold=\u001b[32m0.001\u001b[39m,\n\u001b[32m     67\u001b[39m     )\n\u001b[32m     68\u001b[39m )\n\u001b[32m     69\u001b[39m trainer.remove_callback(PrinterCallback)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/peft/peft_model.py:1923\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1922\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1934\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1936\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:308\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:477\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[32m    480\u001b[39m     loss=loss,\n\u001b[32m    481\u001b[39m     logits=logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    484\u001b[39m     attentions=outputs.attentions,\n\u001b[32m    485\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/loss/loss_utils.py:67\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     66\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/transformers/loss/loss_utils.py:36\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     29\u001b[39m     source: torch.Tensor,\n\u001b[32m     30\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     **kwargs,\n\u001b[32m     34\u001b[39m ) -> torch.Tensor:\n\u001b[32m     35\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(num_items_in_batch):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.13/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 31.36 GiB of which 5.31 GiB is free. Including non-PyTorch memory, this process has 26.04 GiB memory in use. Of the allocated memory 24.96 GiB is allocated by PyTorch, and 512.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_idx = 0\n",
    "sample_row = train_df.iloc[sample_idx]\n",
    "actual_text = template.format(english_text=sample_row['EN'], spanish_text=sample_row['ES'])\n",
    "print(\"Actual text:\", actual_text)\n",
    "\n",
    "sample = train_dataset[sample_idx]\n",
    "trimmed_input_ids = [tok_id for tok_id in sample[\"input_ids\"].tolist() if tok_id != tokenizer.pad_token_id]\n",
    "decoded_text = tokenizer.decode(trimmed_input_ids, skip_special_tokens=False)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# label\n",
    "trimmed_label_ids = [tok_id for tok_id in sample[\"labels\"].tolist() if tok_id != -100]\n",
    "decoded_label_text = tokenizer.decode(trimmed_label_ids, skip_special_tokens=False)\n",
    "print(\"Decoded label text:\", decoded_label_text)\n",
    "\n",
    "print(\"Match:\", actual_text.strip() == decoded_text.strip())\n",
    "print(\"Label Match:\", sample_row['ES'].strip() == decoded_label_text.replace(\"<|END|>\", \"\").strip())\n",
    "\n",
    "\n",
    "cpu_count = os.cpu_count() or 1\n",
    "dataloader_workers = max(cpu_count - 1, 1)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"exp-data/runs/{run_name}\",\n",
    "    run_name=run_name,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=accumulate_grad_batches,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=f\"exp-data/runs/{run_name}/logs\",\n",
    "    logging_steps=1000,\n",
    "    report_to=[\"wandb\"],\n",
    "    max_grad_norm=grad_clip_val,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    dataloader_num_workers=dataloader_workers,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    disable_tqdm=False,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "if preview_samples:\n",
    "    trainer.add_callback(SampleTranslationCallback(tokenizer, preview_samples, max_new_tokens=80))\n",
    "\n",
    "trainer.add_callback(\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001,\n",
    "    )\n",
    ")\n",
    "trainer.remove_callback(PrinterCallback)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59120fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_path = trainer.state.best_model_checkpoint\n",
    "# print(\"Best model saved at:\", best_model_path)\n",
    "\n",
    "# best_checkpoint = trainer.state.best_model_checkpoint\n",
    "# if best_checkpoint is not None:\n",
    "#     print(f\"Loading best model from {best_checkpoint}\")\n",
    "#     trainer._load_best_model()\n",
    "# else:\n",
    "#     print(\"No best checkpoint found; evaluating current model weights.\")\n",
    "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "val_loss = eval_results.get(\"eval_loss\")\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "    \n",
    "hf_repo_id = run_name\n",
    "hf_repo_id = f\"leobitz/{hf_repo_id}\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "artifact_dir = Path(\"exp-data/hf-artifacts\") / Path(run_name).name\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = trainer.model\n",
    "tokenizer = trainer.tokenizer\n",
    "\n",
    "model.save_pretrained(artifact_dir)\n",
    "tokenizer.save_pretrained(artifact_dir)\n",
    "model.config.save_pretrained(artifact_dir)\n",
    "\n",
    "api = HfApi(token=hf_token)\n",
    "create_repo(repo_id=hf_repo_id, exist_ok=True, token=hf_token, private=True)\n",
    "api.upload_folder(\n",
    "    repo_id=hf_repo_id,\n",
    "    folder_path=str(artifact_dir),\n",
    "    path_in_repo=\".\",\n",
    "    commit_message=f\"Upload {run_name} Performance {val_loss:.4f}\",\n",
    ")\n",
    "print(f\"Pushed model and tokenizer to https://huggingface.co/{hf_repo_id}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
