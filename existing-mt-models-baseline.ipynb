{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"exp-data/en-es.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3222e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_args = {\n",
    "    \"truncation\": True,\n",
    "    \"padding\": \"longest\",\n",
    "}\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def translate_batch(texts, model, tokenizer, max_text_length=None):\n",
    "    if max_text_length is None:\n",
    "        tok_args = {**tokenizer_args, 'max_length': tokenizer.model_max_length}\n",
    "    else:\n",
    "        tok_args = {**tokenizer_args, 'max_length': max_text_length}\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", **tok_args)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs)\n",
    "    translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098efa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12091fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(references, predictions):\n",
    "    assert len(references) == len(predictions), f\"Length mismatch: {len(references)} vs {len(predictions)}\"\n",
    "    references = [[ref] for ref in references]\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    return {\n",
    "        \"sacrebleu\": sacrebleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "    }\n",
    "\n",
    "def evaluate(dataset: pd.DataFrame, text_col: str, ref_col: str, translator, save_col=None) -> pd.DataFrame:\n",
    "    translations = []\n",
    "    for i in tqdm.tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch_texts = dataset[text_col].iloc[i:i+batch_size].tolist()\n",
    "        batch_translations = translator(batch_texts)\n",
    "        translations.extend([t.strip() for t in batch_translations])\n",
    "\n",
    "    dataset[save_col] = translations\n",
    "    references = dataset[ref_col].tolist()\n",
    "    \n",
    "    result = {}\n",
    "    datasets_names = dataset['dataset'].unique().tolist()\n",
    "    for name in datasets_names:\n",
    "        sub_dataset = dataset[dataset['dataset'] == name].reset_index(drop=True)\n",
    "        sub_references = sub_dataset[ref_col].tolist()\n",
    "        sub_predictions = sub_dataset[save_col].tolist()\n",
    "        result[name] = get_eval_metrics(sub_references, sub_predictions)\n",
    "    result[\"all\"] = get_eval_metrics(references, translations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c29958",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=['corpus-en-es', 'scientific_papers_en_es', 'Document-Translation-en-es', 'medical-translation-test-set']\n",
    "test_df = df[df['dataset'].isin(datasets_names)].reset_index(drop=True)\n",
    "max_per_dataset = 500\n",
    "all_test_df = test_df[test_df['split'] == 'test']\n",
    "test_dfs = []\n",
    "for name in datasets_names:\n",
    "    sub_df = all_test_df[all_test_df['dataset'] == name].reset_index(drop=True)\n",
    "    print(f\"{name}: {len(sub_df)} samples\")\n",
    "    if len(sub_df) > max_per_dataset:\n",
    "        sub_df = sub_df.sample(n=max_per_dataset, random_state=42).reset_index(drop=True)\n",
    "    test_dfs.append(sub_df)\n",
    "test_df = pd.concat(test_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d86e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\", use_safetensors=True).eval().to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_pipe(texts):\n",
    "    return translate_batch(texts, model, tokenizer)\n",
    "\n",
    "# result = evaluate(test_df, \n",
    "#                   text_col='EN', \n",
    "#                   ref_col='ES', \n",
    "#                   translator=temp_pipe, \n",
    "#                   save_col='translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1eff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_prompting(text):\n",
    "    return f\"\"\"\n",
    "    Translate the following English text to Spanish.\".\n",
    "    English: {text}\n",
    "    Spanish:\n",
    "    \"\"\"\n",
    "\n",
    "def few_shot_prompting(text):\n",
    "    prompt = (\n",
    "        \"Translate the following English text to Spanish.\\n\\n\"\n",
    "        \"English: Hello, how are you?\\n\"\n",
    "        \"Spanish: Hola, ¿cómo estás? [END]\\n\\n\"\n",
    "        \"English: What is your name?\\n\"\n",
    "        \"Spanish: ¿Cómo te llamas? [END]\\n\\n\"\n",
    "        f\"English: {text}\\n\"\n",
    "        \"Spanish:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, device, prompts, system_prompt=None, max_new_tokens=50, return_full_text=False, **generate_kwargs):\n",
    "    if system_prompt == None:\n",
    "        system_prompt = \"You are a helpful assistant that completes sentences.\"\n",
    "    prompts = [f\"{system_prompt}\\n\\n{prompt}\".strip() for prompt in prompts]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512, padding_side='left').to(device)\n",
    "    temperature = generate_kwargs.pop(\"temperature\", 0.7)\n",
    "    do_sample = temperature > 0.0\n",
    "    generate_kwargs.update({\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": do_sample,\n",
    "    })\n",
    "    output_ids = model.generate(**inputs, tokenizer=tokenizer, max_new_tokens=max_new_tokens, **generate_kwargs)\n",
    "    \n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        prompt_length = inputs[\"input_ids\"].shape[-1]\n",
    "        if not return_full_text:\n",
    "            generated_tokens = output_ids[i][prompt_length:]\n",
    "        else:\n",
    "            generated_tokens = output_ids[i]\n",
    "        results.append(tokenizer.decode(generated_tokens, skip_special_tokens=True).strip())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53523135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     device=device, \n",
    "#     prompts=[plain_prompting(\"my name is John.\")],\n",
    "#     system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "#     max_new_tokens=1024, \n",
    "#     temperature=0.0,\n",
    "#     repetition_penalty=1.2,\n",
    "#     stop_strings=[\"END\"],\n",
    "#     top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ec380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     device=device, \n",
    "#     prompts=[few_shot_prompting(\"my name is John.\")],\n",
    "#     system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "#     max_new_tokens=1024, \n",
    "#     temperature=0.0,\n",
    "#     repetition_penalty=1.2,\n",
    "#     stop_strings=[\"[END]\"],\n",
    "#     top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_pipe(texts):\n",
    "    return generate(model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        device=device, \n",
    "        prompts=[few_shot_prompting(text) for text in texts],\n",
    "        system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "        max_new_tokens=1024, \n",
    "        temperature=0.0,\n",
    "        repetition_penalty=1.2,\n",
    "        stop_strings=[\"[END]\"],\n",
    "        top_p=1.0)\n",
    "\n",
    "result = evaluate(test_df, \n",
    "                  text_col='EN', \n",
    "                  ref_col='ES', \n",
    "                  translator=temp_pipe, \n",
    "                  save_col='translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80234450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
