{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14914f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Map: 100%|██████████| 50061/50061 [00:04<00:00, 10425.96 examples/s]\n",
      "Map: 100%|██████████| 50061/50061 [00:04<00:00, 10425.96 examples/s]\n",
      "Map: 100%|██████████| 4939/4939 [00:00<00:00, 10461.34 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['dataset', 'split', 'EN', 'ES', 'length', 'text'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50061/50061 [00:03<00:00, 15938.90 examples/s]\n",
      "Map: 100%|██████████| 50061/50061 [00:03<00:00, 15938.90 examples/s]\n",
      "Map: 100%|██████████| 4939/4939 [00:00<00:00, 16213.68 examples/s]\n",
      "\n",
      "Truncating train dataset: 100%|██████████| 50061/50061 [00:00<00:00, 1442386.26 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 50061/50061 [00:00<00:00, 1442386.26 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 4939/4939 [00:00<00:00, 976601.33 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# train_en_to_es_sft.py\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# ==========================\n",
    "# Config\n",
    "# ==========================\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "DATASET_PATH = \"exp-data/en-es-train-val.parquet\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "OUTPUT_DIR = \"./smollm2-135m-en-es-lora\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# ==========================\n",
    "# Load tokenizer and model (we'll use LoRA so base model weights stay untouched)\n",
    "# ==========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Load and prepare dataset\n",
    "# ==========================\n",
    "dataset = load_dataset(\"parquet\", data_files=DATASET_PATH)[\"train\"]\n",
    "# use 50%\n",
    "dataset = dataset.train_test_split(test_size=0.5, seed=42)[\"train\"]\n",
    "# Split into train/val based on 'split' column\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset   = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Instruction template for translation\n",
    "INSTRUCTION = \"English: {en} Spanish:\"\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    text = INSTRUCTION.format(en=example[\"EN\"]) + \" \" + example[\"ES\"] + tokenizer.eos_token\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func)\n",
    "val_dataset   = val_dataset.map(formatting_prompts_func)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,  # Will be handled by DataCollatorForLanguageModeling\n",
    "    )\n",
    "\n",
    "example_samples = val_dataset.select(range(3))\n",
    "print(example_samples)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset   = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "# ==========================\n",
    "# Custom callback to print 3 samples every epoch\n",
    "# ==========================\n",
    "from transformers import TrainerCallback\n",
    "from peft import PeftModel\n",
    "\n",
    "class TranslationEvalCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, val_dataset, num_samples=3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.val_dataset = val_dataset.select(range(min(num_samples * 10, len(val_dataset))))  # small pool\n",
    "        self.num_samples = num_samples\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.base_model.eval()\n",
    "\n",
    "    def generate_translation(self, model, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "\n",
    "            )\n",
    "        full_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        # Extract only the Spanish part after \"Spanish:\"\n",
    "        try:\n",
    "            spanish = full_text.split(\"Spanish:\")[1].strip().split(tokenizer.eos_token)[0].strip()\n",
    "        except:\n",
    "            spanish = full_text.split(\"Spanish:\")[1].strip() if \"Spanish:\" in full_text else \"ERROR\"\n",
    "        return spanish\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model.eval()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"END OF EPOCH {state.epoch:.1f} - SAMPLE TRANSLATIONS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "        for i, sample in enumerate(example_samples, 1):\n",
    "            prompt = INSTRUCTION.format(en=sample[\"EN\"])\n",
    "\n",
    "            base_pred = self.generate_translation(self.base_model, prompt)\n",
    "            current_pred = self.generate_translation(model, prompt)\n",
    "            reference = sample[\"ES\"]\n",
    "\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"EN → {sample['EN']}\")\n",
    "            print(f\"REF → {reference}\")\n",
    "            # print(f\"BASE (SmolLM2-135M) → {base_pred}\")\n",
    "            print(f\"CURRENT (Fine-tuned) → {current_pred}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "# ==========================\n",
    "# Training arguments\n",
    "# ==========================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you use it\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# SFTTrainer\n",
    "# ==========================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    # max_seq_length=MAX_SEQ_LENGTH,\n",
    "    # dataset_text_field=\"text\",  # We already have tokenized input_ids, but SFTTrainer can handle it\n",
    "    # packing=False,\n",
    "    callbacks=[TranslationEvalCallback(tokenizer, val_dataset, num_samples=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74af9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged full model saved to ./smollm2-135m-en-es-lora-merged\n"
     ]
    }
   ],
   "source": [
    "# Reload base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top\n",
    "lora_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "\n",
    "# Merge and unload adapters\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save merged full model\n",
    "MERGED_OUTPUT_DIR = OUTPUT_DIR + \"-merged\"\n",
    "merged_model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "\n",
    "print(f\"Merged full model saved to {MERGED_OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
