{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccbd37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " `num_layers` is part of LlamaModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/leo/project/distil-research/smol/llama.py.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import LlamaForCausalLM\n",
    "from llama import MultiTaskLlamaForCausalLM\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing as mp\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoConfig\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c75edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-task-True-frozen-8-task-layers-2\n"
     ]
    }
   ],
   "source": [
    "num_frozen_layers = 8\n",
    "num_task_layers = 2\n",
    "max_length = 128\n",
    "effective_batch_size = 32\n",
    "batch_size = 32\n",
    "accumulate_grad_batches = effective_batch_size // batch_size\n",
    "weight_decay = 0.01\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "grad_clip_val = 1.0\n",
    "max_train_sample_size = 100_000\n",
    "max_val_sample_size = 10_000\n",
    "model_name_or_path = \"HuggingFaceTB/SmolLM-135M\"\n",
    "use_lora = False\n",
    "use_qlora = False\n",
    "is_multi_task = (not use_lora)  and (not use_qlora)\n",
    "run_name = f\"multi-task-{is_multi_task}-frozen-{num_frozen_layers}-task-layers-{num_task_layers}\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258b8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "config.num_frozen_layers = num_frozen_layers\n",
    "config.num_task_layers = num_task_layers\n",
    "\n",
    "if not use_lora and not use_qlora:\n",
    "    pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = MultiTaskLlamaForCausalLM(config)\n",
    "    model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    for param in model.model.parameters():\n",
    "        param.requires_grad = False\n",
    "elif use_lora or use_qlora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    if use_qlora:\n",
    "        # For QLoRA, quantize the model weights to 4-bit\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path, quantization_config=quantization_config)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    else:\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"Model updated with LoRA/QLoRA for parameter-efficient fine-tuning.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc299d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in pretrained model: 134,515,008\n",
      "Total number of parameters in multi-task model: 63,713,088\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(pretrained_model)\n",
    "print(f\"Total number of parameters in pretrained model: {total_params:,}\")\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total number of parameters in multi-task model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39177e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples after length filtering: 124417\n",
      "Number of samples: 99533\n",
      "Number of validation samples: 10000\n"
     ]
    }
   ],
   "source": [
    "template = \"English: {english_text} Spanish: {spanish_text} <|END|>\"\n",
    "\n",
    "# exp-data/en-es-train-val.parquet\n",
    "if not os.path.exists(\"exp-data/en-es-train-val.parquet\"):\n",
    "    data_df = pd.read_parquet(\"exp-data/en-es.parquet\")\n",
    "    train_df = data_df[data_df[\"split\"] == \"train\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    train_df['length'] = train_df.apply(lambda x: len(tokenizer.encode(template.format(english_text=x['EN'], spanish_text=x['ES']))), axis=1)\n",
    "    train_df = train_df[train_df['length'] <= max_length].reset_index(drop=True)\n",
    "    print(f\"Number of training samples after length filtering: {len(train_df)}\")\n",
    "    # split train_df 80-20 into train and validation\n",
    "    split_idx = int(0.8 * len(train_df))\n",
    "    train_df_split = train_df.iloc[:split_idx].reset_index(drop=True)\n",
    "    val_df_split = train_df.iloc[split_idx:].reset_index(drop=True)\n",
    "    train_df = train_df_split\n",
    "    val_df = val_df_split\n",
    "    train_df = train_df.sample(min(max_train_sample_size, len(train_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df = val_df.sample(min(max_val_sample_size, len(val_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df['split'] = 'val'\n",
    "    train_df['split'] = 'train'\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")\n",
    "    # to parquet\n",
    "    train_proc_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "    train_proc_df.to_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "else:\n",
    "    train_proc_df = pd.read_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "    train_df = train_proc_df[train_proc_df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "    val_df = train_proc_df[train_proc_df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffd1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.template = \"English: {} Spanish:\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_text = self.df.iloc[idx][\"EN\"]\n",
    "        es_text = self.df.iloc[idx][\"ES\"]\n",
    "        prompt = self.template.format(en_text)\n",
    "        full_text = template.format(english_text=en_text, spanish_text=es_text)\n",
    "\n",
    "        # Tokenize prompt and full text\n",
    "        prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "        full_ids = self.tokenizer(full_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        # Labels: -100 for prompt, actual tokens for output\n",
    "        labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]\n",
    "        input_ids = full_ids\n",
    "\n",
    "        # Pad if necessary\n",
    "        pad_len = self.max_length - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [-100] * pad_len\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TranslationDataset(train_df, tokenizer, max_length=max_length)\n",
    "val_dataset = TranslationDataset(val_df, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97845a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text: English: Women are not even allowed to take part in the distribution of bread under the UN world food programme. Spanish: Las mujeres no pueden participar ni siquiera en la distribuci贸n de pan del Programa de Alimentaci贸n Mundial de la ONU. <|END|>\n",
      "Decoded text: English: Women are not even allowed to take part in the distribution of bread under the UN world food programme. Spanish: Las mujeres no pueden participar ni siquiera en la distribuci贸n de pan del Programa de Alimentaci贸n Mundial de la ONU. <|END|>\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "sample_row = train_df.iloc[sample_idx]\n",
    "actual_text = f\"{train_dataset.template.format(sample_row['EN'])} {sample_row['ES']} <|END|>\"\n",
    "print(\"Actual text:\", actual_text)\n",
    "\n",
    "sample = train_dataset[sample_idx]\n",
    "trimmed_input_ids = [tok_id for tok_id in sample[\"input_ids\"].tolist() if tok_id != tokenizer.pad_token_id]\n",
    "decoded_text = tokenizer.decode(trimmed_input_ids, skip_special_tokens=False)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# label\n",
    "trimmed_label_ids = [tok_id for tok_id in sample[\"labels\"].tolist() if tok_id != -100]\n",
    "decoded_label_text = tokenizer.decode(trimmed_label_ids, skip_special_tokens=False)\n",
    "print(\"Decoded label text:\", decoded_label_text)\n",
    "\n",
    "print(\"Match:\", actual_text.strip() == decoded_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example using the model and tokenizer\n",
    "en_sentence = train_df['EN'][0]\n",
    "prompt = f\"English: {en_sentence} Spanish:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "# Move input_ids to the same device as the model if needed\n",
    "device = next(model.parameters()).device\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Decode the generated Spanish translation\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = mp.cpu_count() - 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitTranslationModel(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer, lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        return self.model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader))\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    dirpath=f\"exp-data/runs/{run_name}/checkpoints\",\n",
    "    filename=\"best-{epoch:02d}-{val_loss:.4f}\"\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(name=\"bitzbrain\", project=\"en-es\", log_model=\"all\")\n",
    "\n",
    "lit_model = LitTranslationModel(model, tokenizer, lr=learning_rate)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        max_epochs=epochs, \n",
    "        accelerator=\"auto\", \n",
    "        devices=\"auto\",\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        gradient_clip_val=grad_clip_val,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=wandb_logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfa6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
