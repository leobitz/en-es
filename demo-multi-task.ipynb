{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccbd37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ `num_layers` is part of LlamaModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /workspace/en-es/llama.py.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu\n",
    "import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import LlamaForCausalLM\n",
    "from llama import MultiTaskLlamaForCausalLM\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing as mp\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoConfig\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c75edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-task-True-frozen-8-task-layers-2\n"
     ]
    }
   ],
   "source": [
    "num_frozen_layers = 8\n",
    "num_task_layers = 2\n",
    "max_length = 128\n",
    "effective_batch_size = 32\n",
    "batch_size = 32\n",
    "accumulate_grad_batches = effective_batch_size // batch_size\n",
    "weight_decay = 0.01\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "grad_clip_val = 1.0\n",
    "max_train_sample_size = 100_000\n",
    "max_val_sample_size = 10_000\n",
    "model_name_or_path = \"HuggingFaceTB/SmolLM-135M\"\n",
    "use_lora = False\n",
    "use_qlora = False\n",
    "is_multi_task = (not use_lora)  and (not use_qlora)\n",
    "run_name = f\"multi-task-{is_multi_task}-frozen-{num_frozen_layers}-task-layers-{num_task_layers}\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258b8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "config.num_frozen_layers = num_frozen_layers\n",
    "config.num_task_layers = num_task_layers\n",
    "\n",
    "if not use_lora and not use_qlora:\n",
    "    pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = MultiTaskLlamaForCausalLM(config)\n",
    "    model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    for param in model.model.parameters():\n",
    "        param.requires_grad = False\n",
    "elif use_lora or use_qlora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    if use_qlora:\n",
    "        # For QLoRA, quantize the model weights to 4-bit\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path, quantization_config=quantization_config)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    else:\n",
    "        pretrained_model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "        model = MultiTaskLlamaForCausalLM(config)\n",
    "        model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"Model updated with LoRA/QLoRA for parameter-efficient fine-tuning.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc299d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in pretrained model: 134,515,008\n",
      "Total number of parameters in multi-task model: 63,713,088\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params = count_parameters(pretrained_model)\n",
    "print(f\"Total number of parameters in pretrained model: {total_params:,}\")\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total number of parameters in multi-task model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39177e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of validation samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# exp-data/en-es-train-val.parquet\n",
    "if not os.path.exists(\"exp-data/en-es-train-val.parquet\"):\n",
    "    data_df = pd.read_parquet(\"exp-data/en-es.parquet\")\n",
    "    train_df = data_df[data_df[\"split\"] == \"train\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    train_df['length'] = train_df['ES'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    train_df = train_df[train_df['length'] <= max_length].reset_index(drop=True)\n",
    "    print(f\"Number of training samples after length filtering: {len(train_df)}\")\n",
    "    # split train_df 80-20 into train and validation\n",
    "    split_idx = int(0.8 * len(train_df))\n",
    "    train_df_split = train_df.iloc[:split_idx].reset_index(drop=True)\n",
    "    val_df_split = train_df.iloc[split_idx:].reset_index(drop=True)\n",
    "    train_df = train_df_split\n",
    "    val_df = val_df_split\n",
    "    train_df = train_df.sample(min(max_train_sample_size, len(train_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df = val_df.sample(min(max_val_sample_size, len(val_df)), random_state=42).reset_index(drop=True)\n",
    "    val_df['split'] = 'val'\n",
    "    train_df['split'] = 'train'\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")\n",
    "    # to parquet\n",
    "    train_proc_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "    train_proc_df.to_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "else:\n",
    "    train_proc_df = pd.read_parquet(\"exp-data/en-es-train-val.parquet\")\n",
    "    train_df = train_proc_df[train_proc_df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "    val_df = train_proc_df[train_proc_df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "    print(f\"Number of samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffd1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.template = \"English: {} Spanish:\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_text = self.df.iloc[idx][\"EN\"]\n",
    "        es_text = self.df.iloc[idx][\"ES\"]\n",
    "        prompt = self.template.format(en_text)\n",
    "        full_text = f\"{prompt} {es_text}\"\n",
    "\n",
    "        # Tokenize prompt and full text\n",
    "        prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "        full_ids = self.tokenizer(full_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        # Labels: -100 for prompt, actual tokens for output\n",
    "        labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]\n",
    "        input_ids = full_ids\n",
    "\n",
    "        # Pad if necessary\n",
    "        pad_len = self.max_length - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [-100] * pad_len\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TranslationDataset(train_df, tokenizer, max_length=max_length)\n",
    "val_dataset = TranslationDataset(val_df, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69a9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: English: Frederick V or Friedrich V may refer to: Frederick V, Duke of Swabia (1164â€“1170) Frederick V, Count of Zollern (d.1289) Frederick V, Burgrave of Nuremberg (c. 1333â€“1398), German noble Frederick V of Austria (1415â€“1493), or Frederick III, Holy Roman Emperor Frederick I, Margrave of Brandenburg-Ansbach (1460â€“1536), or Friedrich V, Margrave von Brandenburg-Ansbach-Bayreuth Frederick V, Margrave of Baden-Durlach (1594â€“1659) Frederick V, Elector Palatine (1596â€“1632), or Friedrich V von der Pfalz Frederick V of Denmark (1723â€“1766), king of Denmark and Norway Frederick V, Landgrave of Hesse-Homburg (1748â€“1820) Spanish:\n",
      "Generated: English: Frederick V or Friedrich V may refer to: Frederick V, Duke of Swabia (1164â€“1170) Frederick V, Count of Zollern (d.1289) Frederick V, Burgrave of Nuremberg (c. 1333â€“1398), German noble Frederick V of Austria (1415â€“1493), or Frederick III, Holy Roman Emperor Frederick I, Margrave of Brandenburg-Ansbach (1460â€“1536), or Friedrich V, Margrave von Brandenburg-Ansbach either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either either\n"
     ]
    }
   ],
   "source": [
    "# Inference example using the model and tokenizer\n",
    "en_sentence = train_df['EN'][0]\n",
    "prompt = f\"English: {en_sentence} Spanish:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "# Move input_ids to the same device as the model if needed\n",
    "device = next(model.parameters()).device\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Decode the generated Spanish translation\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d41fbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = mp.cpu_count() - 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca1c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "class LitTranslationModel(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer, lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        return self.model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader))\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    dirpath=f\"exp-data/runs/{run_name}/checkpoints\",\n",
    "    filename=\"best-{epoch:02d}-{val_loss:.4f}\"\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(name=\"bitzbrain\", project=\"en-es\", log_model=\"all\")\n",
    "\n",
    "lit_model = LitTranslationModel(model, tokenizer, lr=learning_rate)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        max_epochs=epochs, \n",
    "        accelerator=\"auto\", \n",
    "        devices=\"auto\",\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        gradient_clip_val=grad_clip_val,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=wandb_logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfa6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleobitz\u001b[0m (\u001b[33mbitzbrain\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251124_005549-b1h7ow5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bitzbrain/en-es/runs/b1h7ow5b' target=\"_blank\">bitzbrain</a></strong> to <a href='https://wandb.ai/bitzbrain/en-es' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bitzbrain/en-es' target=\"_blank\">https://wandb.ai/bitzbrain/en-es</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bitzbrain/en-es/runs/b1h7ow5b' target=\"_blank\">https://wandb.ai/bitzbrain/en-es/runs/b1h7ow5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | MultiTaskLlamaForCausalLM | 63.7 M | train\n",
      "------------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "56.6 M    Non-trainable params\n",
      "63.7 M    Total params\n",
      "254.852   Total estimated model params size (MB)\n",
      "138       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b4943d331a4bf0a7eaa79f02930c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d88726068e440959dd9709c2e628d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8217b99e6494a6aa31e669c56161439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 3.998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630506ea6e8645fb9b20b0a6e7f4b68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.365 >= min_delta = 0.001. New best score: 3.633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6effcb6ed2a64c358da905050e87a03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.186 >= min_delta = 0.001. New best score: 3.447\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
