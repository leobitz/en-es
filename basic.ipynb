{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6340a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic greedy generation helper tailored for prompting workflows.\n",
    "def generate(model, tokenizer, device, prompts, system_prompt=None, max_new_tokens=50, return_full_text=False, **generate_kwargs):\n",
    "    if system_prompt == None:\n",
    "        system_prompt = \"You are a helpful assistant that completes sentences.\"\n",
    "    prompts = [f\"{system_prompt}\\n\\n{prompt}\".strip() for prompt in prompts]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512, padding_side='left').to(device)\n",
    "    temprature = generate_kwargs.pop(\"temperature\", 0.7)\n",
    "    do_sample = temprature > 0.0\n",
    "    generate_kwargs.update({\n",
    "        \"temperature\": temprature,\n",
    "        \"do_sample\": do_sample,\n",
    "    })\n",
    "    output_ids = model.generate(**inputs, tokenizer=tokenizer, max_new_tokens=max_new_tokens, **generate_kwargs)\n",
    "    \n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        prompt_length = inputs[\"input_ids\"].shape[-1]\n",
    "        if not return_full_text:\n",
    "            generated_tokens = output_ids[i][prompt_length:]\n",
    "        else:\n",
    "            generated_tokens = output_ids[i]\n",
    "        results.append(tokenizer.decode(generated_tokens, skip_special_tokens=True).strip())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model=model, \n",
    "         tokenizer=tokenizer, \n",
    "         device=device, \n",
    "         prompts=[\"Once upon a time in\", \"In a galaxy far, far away\"], \n",
    "         max_new_tokens=20,\n",
    "         temperature=0.0,\n",
    "         system_prompt=\"You are a helpful assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"loresiensis/corpus-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate sacrebleu\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"en\": \"Hello, how are you?\",\n",
    "        \"es\": \"Hola, ¿cómo estás? END\"\n",
    "    },\n",
    "    {\n",
    "        \"en\": \"What is your name?\",\n",
    "        \"es\": \"¿Cuál es tu nombre? END\"\n",
    "    },\n",
    "    {\n",
    "        \"en\": \"I love programming.\",\n",
    "        \"es\": \"Me encanta programar. END\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "examples_str = \"\"\n",
    "for ex in examples:\n",
    "    examples_str += f\"English: {ex['en']}\\nSpanish: {ex['es']}\\n\\n\"\n",
    "instruction_prompt = \"Translate the following English sentences to Spanish and end each translation with the token END.\\n\\n\"\n",
    "def prompt_translate_en_to_es(text):\n",
    "    prompt = (\n",
    "        # f\"{examples_str}\",\n",
    "        instruction_prompt,\n",
    "        f\"English: {text.strip()}\\n\"\n",
    "        \"Spanish:\"\n",
    "    )\n",
    "    prompt = \"\".join(prompt)\n",
    "    return prompt\n",
    "\n",
    "prompt_translate_en_to_es(\"My name is John.\")  # Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569692c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=device, \n",
    "    prompts=[prompt_translate_en_to_es(\"my name is John.\")],\n",
    "    system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "    max_new_tokens=1024, \n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.2,\n",
    "    stop_strings=[\"END\"],\n",
    "    top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70973f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation_split(split, max_samples=100, batch_size=16):\n",
    "    if max_samples is not None:\n",
    "        subset_size = min(len(split), max_samples)\n",
    "    else:\n",
    "        subset_size = len(split)\n",
    "\n",
    "    # Get lists of source texts and references\n",
    "    src_texts = split[\"EN\"][:subset_size]\n",
    "    tgt_texts = split[\"ES\"][:subset_size]\n",
    "\n",
    "    predictions = []\n",
    "    # run generation in batches for efficiency\n",
    "    for i in tqdm(range(0, subset_size, batch_size), desc=\"Translating EN→ES\"):\n",
    "        batch_texts = src_texts[i : i + batch_size]\n",
    "        prompts = [prompt_translate_en_to_es(text) for text in batch_texts]\n",
    "        \n",
    "        # The `generate` function is already set up to handle a list of prompts\n",
    "        batch_preds = generate(model=model, \n",
    "                               tokenizer=tokenizer, \n",
    "                               device=device, \n",
    "                               prompts=prompts,\n",
    "                               system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "                               max_new_tokens=1024, \n",
    "                               temperature=0.0,\n",
    "                               repetition_penalty=1.2,\n",
    "                               stop_strings=[\"END\"],\n",
    "                               top_p=1.0)\n",
    "        \n",
    "        # The result from `generate` is a list of strings, which we can extend our predictions with\n",
    "        predictions.extend([p.strip(\"END\").strip() for p in batch_preds])\n",
    "\n",
    "    # prepare references in the format expected by the metrics: list[list[str]]\n",
    "    references = [[r] for r in tgt_texts]\n",
    "\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "\n",
    "    return {\n",
    "        \"num_eval_samples\": subset_size,\n",
    "        \"sacrebleu\": sacrebleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eval_metrics = evaluate_translation_split(ds[\"test\"], max_samples=100)\n",
    "# test_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6801791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43505644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_translation(split, src=\"ES\", tgt=\"EN\", max_samples=100, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the existing `pipe` translation pipeline on a dataset split.\n",
    "    Assumes `pipe`, `sacrebleu`, and `chrf` are already available in the notebook.\n",
    "    By default evaluates ES -> EN (src=\"ES\", tgt=\"EN\"). Adjust src/tgt as needed.\n",
    "    \"\"\"\n",
    "    if max_samples is not None:\n",
    "        subset_size = min(len(split), max_samples)\n",
    "    else:\n",
    "        subset_size = len(split)\n",
    "\n",
    "    # Get lists of source texts and references\n",
    "    src_texts = split[src][:subset_size]\n",
    "    tgt_texts = split[tgt][:subset_size]\n",
    "\n",
    "    predictions = []\n",
    "    # run pipeline in batches for efficiency\n",
    "    for i in tqdm(range(0, subset_size, batch_size), desc=\"Translating with pipeline\"):\n",
    "        batch_texts = src_texts[i : i + batch_size]\n",
    "        outputs = pipe(batch_texts)  # pipeline returns list of dicts with 'translation_text'\n",
    "        batch_preds = [out[\"translation_text\"].strip() for out in outputs]\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "    # prepare references in the format expected by the metrics: list[list[str]]\n",
    "    references = [[r] for r in tgt_texts]\n",
    "\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "\n",
    "    return {\n",
    "        \"num_eval_samples\": subset_size,\n",
    "        \"sacrebleu\": sacrebleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"example_predictions\": predictions[:5],  # a few examples for quick inspection\n",
    "    }\n",
    "\n",
    "# Example usage on the test split (adjust max_samples/batch_size as desired)\n",
    "pipeline_test_metrics = evaluate_pipeline_translation(ds[\"test\"], src=\"EN\", tgt=\"ES\", max_samples=100, batch_size=32)\n",
    "pipeline_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c31fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "# model = PeftModel.from_pretrained(model, \"outputs/smol-lora/lora-adapter/\")\n",
    "\n",
    "# # Now we can evaluate the LoRA model using the same function as before\n",
    "# lora_test_eval_metrics = evaluate_translation_split(ds[\"test\"], max_samples=100)\n",
    "# lora_test_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99161102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'num_eval_samples': 100,\n",
    "#  'sacrebleu': 4.738854543392033,\n",
    "#  'chrf': 32.4413022633244}\n",
    "\n",
    "# {'num_eval_samples': 100,\n",
    "#  'sacrebleu': 7.065010142972971,\n",
    "#  'chrf': 37.08565733996998}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"  # or any non-instruct HF model\n",
    "\n",
    "llm = LLM(model=model_name)\n",
    "sampling_params = SamplingParams(max_tokens=512, temperature=0.0, top_p=1.0, repetition_penalty=1.2, stop_strings=[\"END\"])\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time in Ethiopia,\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ca75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time in Ethiopia,\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f09615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
