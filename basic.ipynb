{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11c1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbf570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic greedy generation helper tailored for prompting workflows.\n",
    "def generate(model, tokenizer, device, prompts, system_prompt=None, max_new_tokens=50, return_full_text=False, **generate_kwargs):\n",
    "    if system_prompt == None:\n",
    "        system_prompt = \"You are a helpful assistant that completes sentences.\"\n",
    "    prompts = [f\"{system_prompt}\\n\\n{prompt}\".strip() for prompt in prompts]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512, padding_side='left').to(device)\n",
    "    temprature = generate_kwargs.pop(\"temperature\", 0.7)\n",
    "    do_sample = temprature > 0.0\n",
    "    generate_kwargs.update({\n",
    "        \"temperature\": temprature,\n",
    "        \"do_sample\": do_sample,\n",
    "    })\n",
    "    output_ids = model.generate(**inputs, tokenizer=tokenizer, max_new_tokens=max_new_tokens, **generate_kwargs)\n",
    "    \n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        prompt_length = inputs[\"input_ids\"].shape[-1]\n",
    "        if not return_full_text:\n",
    "            generated_tokens = output_ids[i][prompt_length:]\n",
    "        else:\n",
    "            generated_tokens = output_ids[i]\n",
    "        results.append(tokenizer.decode(generated_tokens, skip_special_tokens=True).strip())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc2b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a small town, there was a curious little girl named Lily. She loved exploring her surroundings and learning',\n",
       " ', there was a wise old scientist named Professor Einstein. He lived in a beautiful planet called Earth,']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model, \n",
    "         tokenizer=tokenizer, \n",
    "         device=device, \n",
    "         prompts=[\"Once upon a time in\", \"In a galaxy far, far away\"], \n",
    "         max_new_tokens=20,\n",
    "         temperature=0.0,\n",
    "         system_prompt=\"You are a helpful assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbbd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"loresiensis/corpus-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a1bc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the following English sentences to Spanish and end each translation with the token END.\\n\\nEnglish: My name is John.\\nSpanish:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install evaluate sacrebleu\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"en\": \"Hello, how are you?\",\n",
    "        \"es\": \"Hola, ¿cómo estás? END\"\n",
    "    },\n",
    "    {\n",
    "        \"en\": \"What is your name?\",\n",
    "        \"es\": \"¿Cuál es tu nombre? END\"\n",
    "    },\n",
    "    {\n",
    "        \"en\": \"I love programming.\",\n",
    "        \"es\": \"Me encanta programar. END\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "examples_str = \"\"\n",
    "for ex in examples:\n",
    "    examples_str += f\"English: {ex['en']}\\nSpanish: {ex['es']}\\n\\n\"\n",
    "instruction_prompt = \"Translate the following English sentences to Spanish and end each translation with the token END.\\n\\n\"\n",
    "def prompt_translate_en_to_es(text):\n",
    "    prompt = (\n",
    "        # f\"{examples_str}\",\n",
    "        instruction_prompt,\n",
    "        f\"English: {text.strip()}\\n\"\n",
    "        \"Spanish:\"\n",
    "    )\n",
    "    prompt = \"\".join(prompt)\n",
    "    return prompt\n",
    "\n",
    "prompt_translate_en_to_es(\"My name is John.\")  # Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569692c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"¿cómo estás?\\n(Translation) My name is John, I am your friend. (Informal)\\nMy name is John, you're welcome! (Formal)\\n\\n1. My name is John.\\n2. My name is John. You're welcome!\\n3. My name is John.\\n4. My name is John. You're welcome!\\n5. My name is John.\\n6. My name is John. You're welcome!\\n7. My name is John.\\n8. My name is John. You're welcome!\\n9. My name is John.\\n10. My name is John. You're welcome!\\n11. My name is John.\\n12. My name is John. You're welcome!\\n13. My name is John.\\n14. My name is John. You're welcome!\\n15. My name is John.\\n16. My name is John. You're welcome!\\n17. My name is John.\\n18. My name is John. You're welcome!\\n19. My name is John.\\n20. My name is John. You're welcome!\\n21. My name is John.\\n22. My name is John. You're welcome!\\n23. My name is John.\\n24. My name is John. You're welcome!\\n25. My name is John.\\n26. My name is John. You're welcome!\\n27. My name is John.\\n28. My name is John. You're welcome!\\n29. My name is John.\\n30. My name is John. You're welcome!\\n31. My name is John.\\n32. My name is John. You're welcome!\\n33. My name is John.\\n34. My name is John. You're welcome!\\n35. My name is John.\\n36. My name is John. You're welcome!\\n37. My name is John.\\n38. My name is John. You're welcome!\\n39. My name is John.\\n40. My name is John. You're welcome!\\n41. My name is John.\\n42. My name is John. You're welcome!\\n43. My name is John.\\n44. My name is John. You're welcome!\\n45. My name is John.\\n46. My name is John. You're welcome!\\n47. My name is John.\\n48. My name is John. You're welcome!\\n49. My name is John.\\n50. My name is John. You're welcome!\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=device, \n",
    "    prompts=[prompt_translate_en_to_es(\"my name is John.\")],\n",
    "    system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "    max_new_tokens=1024, \n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.2,\n",
    "    stop_strings=[\"END\"],\n",
    "    top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70973f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation_split(split, max_samples=100, batch_size=16):\n",
    "    if max_samples is not None:\n",
    "        subset_size = min(len(split), max_samples)\n",
    "    else:\n",
    "        subset_size = len(split)\n",
    "\n",
    "    # Get lists of source texts and references\n",
    "    src_texts = split[\"EN\"][:subset_size]\n",
    "    tgt_texts = split[\"ES\"][:subset_size]\n",
    "\n",
    "    predictions = []\n",
    "    # run generation in batches for efficiency\n",
    "    for i in tqdm(range(0, subset_size, batch_size), desc=\"Translating EN→ES\"):\n",
    "        batch_texts = src_texts[i : i + batch_size]\n",
    "        prompts = [prompt_translate_en_to_es(text) for text in batch_texts]\n",
    "        \n",
    "        # The `generate` function is already set up to handle a list of prompts\n",
    "        batch_preds = generate(model=model, \n",
    "                               tokenizer=tokenizer, \n",
    "                               device=device, \n",
    "                               prompts=prompts,\n",
    "                               system_prompt=\"You are a very accurate English to Spanish translator.\", \n",
    "                               max_new_tokens=1024, \n",
    "                               temperature=0.0,\n",
    "                               repetition_penalty=1.2,\n",
    "                               stop_strings=[\"END\"],\n",
    "                               top_p=1.0)\n",
    "        \n",
    "        # The result from `generate` is a list of strings, which we can extend our predictions with\n",
    "        predictions.extend([p.strip(\"END\").strip() for p in batch_preds])\n",
    "\n",
    "    # prepare references in the format expected by the metrics: list[list[str]]\n",
    "    references = [[r] for r in tgt_texts]\n",
    "\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "\n",
    "    return {\n",
    "        \"num_eval_samples\": subset_size,\n",
    "        \"sacrebleu\": sacrebleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b01d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eval_metrics = evaluate_translation_split(ds[\"test\"], max_samples=100)\n",
    "# test_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6801791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43505644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d1b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b584ad4c844b53a63bbd761523dc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating with pipeline:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'num_eval_samples': 100,\n",
       " 'sacrebleu': 44.271848996984225,\n",
       " 'chrf': 66.89272505410729,\n",
       " 'example_predictions': ['Los ciudadanos han perdido la confianza en los sistemas nacionales y europeos de seguridad alimentaria después de los escándalos y los miedos con la carne de vacuno, E.coli, la listeria, la salmonela, las dioxinas, los huevos, las aves de corral, la leche y las hormonas.',\n",
       "  'Me limitaré a añadir a las propuestas de la Comisión de Asuntos Jurídicos, junto con la señora Niebler –la ponente alternativa que realmente ha hecho un excelente trabajo, como quedó claro en su intervención de ayer también– una enmienda que me parece especialmente importante, que pretende aclarar la definición de operador, que, tal como está redactada en la actualidad, no permite excluir a los bancos y otras instituciones financieras implicadas de la responsabilidad por los daños medioambientales causados por los operadores financieros.',\n",
       "  'Algunos de estos derechos ya son \"de aplicación legal\", es decir, pueden constituir una base adecuada para una decisión de un órgano jurídico, sin embargo, no por el hecho de que están incluidos en la Carta de los Derechos Fundamentales, sino porque están tomados de otros instrumentos.',\n",
       "  'Por bien intencionadas que sean, estas propuestas son poco realistas, poco consideradas y socavarían la reputación de Europa de adoptar un enfoque responsable.',\n",
       "  'El pueblo de Guatemala se ha visto obligado a seguir el camino de la lucha armada para defender sus derechos fundamentales.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_pipeline_translation(split, src=\"ES\", tgt=\"EN\", max_samples=100, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the existing `pipe` translation pipeline on a dataset split.\n",
    "    Assumes `pipe`, `sacrebleu`, and `chrf` are already available in the notebook.\n",
    "    By default evaluates ES -> EN (src=\"ES\", tgt=\"EN\"). Adjust src/tgt as needed.\n",
    "    \"\"\"\n",
    "    if max_samples is not None:\n",
    "        subset_size = min(len(split), max_samples)\n",
    "    else:\n",
    "        subset_size = len(split)\n",
    "\n",
    "    # Get lists of source texts and references\n",
    "    src_texts = split[src][:subset_size]\n",
    "    tgt_texts = split[tgt][:subset_size]\n",
    "\n",
    "    predictions = []\n",
    "    # run pipeline in batches for efficiency\n",
    "    for i in tqdm(range(0, subset_size, batch_size), desc=\"Translating with pipeline\"):\n",
    "        batch_texts = src_texts[i : i + batch_size]\n",
    "        outputs = pipe(batch_texts)  # pipeline returns list of dicts with 'translation_text'\n",
    "        batch_preds = [out[\"translation_text\"].strip() for out in outputs]\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "    # prepare references in the format expected by the metrics: list[list[str]]\n",
    "    references = [[r] for r in tgt_texts]\n",
    "\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "\n",
    "    return {\n",
    "        \"num_eval_samples\": subset_size,\n",
    "        \"sacrebleu\": sacrebleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"example_predictions\": predictions[:5],  # a few examples for quick inspection\n",
    "    }\n",
    "\n",
    "# Example usage on the test split (adjust max_samples/batch_size as desired)\n",
    "pipeline_test_metrics = evaluate_pipeline_translation(ds[\"test\"], src=\"EN\", tgt=\"ES\", max_samples=100, batch_size=32)\n",
    "pipeline_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c31fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "# model = PeftModel.from_pretrained(model, \"outputs/smol-lora/lora-adapter/\")\n",
    "\n",
    "# # Now we can evaluate the LoRA model using the same function as before\n",
    "# lora_test_eval_metrics = evaluate_translation_split(ds[\"test\"], max_samples=100)\n",
    "# lora_test_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99161102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'num_eval_samples': 100,\n",
    "#  'sacrebleu': 4.738854543392033,\n",
    "#  'chrf': 32.4413022633244}\n",
    "\n",
    "# {'num_eval_samples': 100,\n",
    "#  'sacrebleu': 7.065010142972971,\n",
    "#  'chrf': 37.08565733996998}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
